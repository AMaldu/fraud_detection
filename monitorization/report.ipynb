{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'evidently.metric_packs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mevidently\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreport\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Report\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mevidently\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ColumnDriftMetric, DatasetDriftMetric, DatasetMissingValuesMetric\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mevidently\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetric_packs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataDriftMetricPack\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load, dump\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm \n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'evidently.metric_packs'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "\n",
    "from evidently import ColumnMapping\n",
    "from evidently.report import Report\n",
    "from evidently.metrics import ColumnDriftMetric, DatasetDriftMetric, DatasetMissingValuesMetric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.pkl', 'rb') as file:\n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy en entrenamiento: 1.00\n",
      "Accuracy en prueba: 1.00\n",
      "Reporte de clasificación para el conjunto de prueba:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1995\n",
      "           1       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           1.00      2000\n",
      "   macro avg       0.50      0.50      0.50      2000\n",
      "weighted avg       1.00      1.00      1.00      2000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maldu/.local/share/virtualenvs/fraud_detection-I9haicwR/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/maldu/.local/share/virtualenvs/fraud_detection-I9haicwR/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/maldu/.local/share/virtualenvs/fraud_detection-I9haicwR/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Cargar el modelo\n",
    "model = joblib.load('model.pkl')\n",
    "\n",
    "# Cargar datos escalados\n",
    "load_dir = '../data/gold'\n",
    "X_train_scaled_npz = np.load(os.path.join(load_dir, 'X_train_scaled.npz'))\n",
    "X_test_scaled_npz = np.load(os.path.join(load_dir, 'X_test_scaled.npz'))\n",
    "\n",
    "# Convertir datos escalados de formato sparse a csr_matrix\n",
    "X_train_scaled_sparse = sp.csr_matrix((X_train_scaled_npz['data'], X_train_scaled_npz['indices'], X_train_scaled_npz['indptr']),\n",
    "                                      shape=X_train_scaled_npz['shape'])\n",
    "X_test_scaled_sparse = sp.csr_matrix((X_test_scaled_npz['data'], X_test_scaled_npz['indices'], X_test_scaled_npz['indptr']),\n",
    "                                     shape=X_test_scaled_npz['shape'])\n",
    "\n",
    "# Cargar etiquetas\n",
    "y_train = joblib.load(os.path.join(load_dir, 'y_train.pkl'))\n",
    "y_test = joblib.load(os.path.join(load_dir, 'y_test.pkl'))\n",
    "\n",
    "# Seleccionar subconjuntos\n",
    "X_train_scaled_subset_sparse = X_train_scaled_sparse[:8000]\n",
    "y_train_subset = y_train[:8000]\n",
    "\n",
    "X_test_scaled_subset_sparse = X_test_scaled_sparse[:2000]\n",
    "y_test_subset = y_test[:2000]\n",
    "\n",
    "# Realizar predicciones con el modelo Random Forest\n",
    "train_preds = model.predict(X_train_scaled_subset_sparse)\n",
    "val_preds = model.predict(X_test_scaled_subset_sparse)\n",
    "\n",
    "# Evaluar el modelo\n",
    "train_accuracy = accuracy_score(y_train_subset, train_preds)\n",
    "print(f'Accuracy en entrenamiento: {train_accuracy:.2f}')\n",
    "\n",
    "val_accuracy = accuracy_score(y_test_subset, val_preds)\n",
    "print(f'Accuracy en prueba: {val_accuracy:.2f}')\n",
    "\n",
    "print('Reporte de clasificación para el conjunto de prueba:')\n",
    "print(classification_report(y_test_subset, val_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = df.select_dtypes(include=['int', 'float'])\n",
    "categorical_features = df.select_dtypes(include=['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_mapping = ColumnMapping(\n",
    "    target = None,\n",
    "    prediction = 'pred',\n",
    "    numerical_features=numerical_features,\n",
    "    categorical_features= categorical_features\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = Report(metrics=[\n",
    "    ColumnDriftMetric(column_name = 'isFraud'),\n",
    "    DatasetDriftMetric(),\n",
    "    DatasetMissingValuesMetric()\n",
    "    \n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataDriftMetricPack' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m column_mapping \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m----> 2\u001b[0m report \u001b[38;5;241m=\u001b[39m Report(metrics\u001b[38;5;241m=\u001b[39m[\u001b[43mDataDriftMetricPack\u001b[49m()])\n\u001b[1;32m      4\u001b[0m X_train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mfrom_spmatrix(X_train_scaled_subset_sparse)\n\u001b[1;32m      5\u001b[0m X_test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39msparse\u001b[38;5;241m.\u001b[39mfrom_spmatrix(X_test_scaled_subset_sparse)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataDriftMetricPack' is not defined"
     ]
    }
   ],
   "source": [
    "column_mapping = {\"target\": \"target\"}\n",
    "report = Report(metrics=[DataDriftMetricPack()])\n",
    "\n",
    "X_train_df = pd.DataFrame.sparse.from_spmatrix(X_train_scaled_subset_sparse)\n",
    "X_test_df = pd.DataFrame.sparse.from_spmatrix(X_test_scaled_subset_sparse)\n",
    "\n",
    "\n",
    "report.run(reference_data=X_train_df, current_data=X_test_df, column_mapping=column_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.show(mode='inline')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "guardar train_data y val_data en cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.run(reference_data=train_data, current_data= val_data, column_mapping=column_mapping)\n",
    "report.show(mode='inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = report.as_dict()\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction drift\n",
    "result['metrics'][0]['result']['drift_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of drifted columns\n",
    "result['metrics'][0]['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#share of missing values\n",
    "result['metrics'][2]['result']['current']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraud_detection-I9haicwR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
